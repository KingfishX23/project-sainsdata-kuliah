{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#* open file csv\n",
    "df = pd.read_csv('TRANSLATED-covid-sentiment.csv')\n",
    "\n",
    "#* ilangin kolom2 berikut\n",
    "columns = ['mentions', 'conversation_id', 'user_id', 'hashtags']\n",
    "df.drop(columns, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a list of imports :D\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords \n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# DATAPREPROCESSING: Data preprocessing and cleaning the tweets dataframe into a tokenized version :D\n",
    "\n",
    "# dd is dask dataframe which utilized multiprocessing, cuz we have the NEED FOR SPEED :O\n",
    "ddf = dd.from_pandas(df, npartitions=30)\n",
    "\n",
    "def preProcess(row):\n",
    "    tweet = row['tweet'].lower() # convert text to lower-case\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet) # remove URLs\n",
    "    tweet = re.sub('@[^\\s]+', 'AT_USER', tweet) # remove usernames\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) # remove the # in #hashtag\n",
    "    \n",
    "    listStopword =  set(stopwords.words('indonesian'))\n",
    "    \n",
    "    removed = []\n",
    "    for t in tweet:\n",
    "        if t not in listStopword:\n",
    "            removed.append(t)\n",
    "\n",
    "    tweet = stemmer.stem(tweet) #remove kata2 imbuhan\n",
    "    tweet = word_tokenize(tweet) #transform the string to an array form\n",
    "    print(tweet)\n",
    "    return tweet\n",
    "\n",
    "res = ddf.map_partitions(lambda df: df.apply(preProcess,axis=1)).compute(scheduler='processes')\n",
    "df[\"tokenized\"] = res\n",
    "df.to_csv('TOKENIZED-covid-sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the amount of words in the whole dataframe :O\n",
    "\n",
    "import ast\n",
    "import dask.dataframe as dd\n",
    "import nltk \n",
    "import itertools\n",
    "\n",
    "df = pd.read_csv('TOKENIZED-covid-sentiment.csv')\n",
    "\n",
    "# dd is dask dataframe which utilized multiprocessing, cuz we wanna know, HOW THEY LIVE IN TOKYO :O\n",
    "ddf = dd.from_pandas(df, npartitions=30)\n",
    "\n",
    "def removeQuotes(row):\n",
    "    def perfectEval(anonstring):\n",
    "        try:\n",
    "            ev = ast.literal_eval(anonstring)\n",
    "            return ev\n",
    "        except ValueError:\n",
    "            corrected = \"\\'\" + anonstring + \"\\'\"\n",
    "            ev = ast.literal_eval(corrected)\n",
    "            return ev\n",
    "    \n",
    "    x = perfectEval(row[\"tokenized\"])\n",
    "    type(x)\n",
    "    return x\n",
    "\n",
    "#NUMERO UNO: because the array is saved as a string, we need to make sure THERE ARE NO STRINGS ATTACHED >:O\n",
    "res = ddf.map_partitions(lambda df: df.apply(removeQuotes, axis=1)).compute(scheduler='processes')\n",
    "df[\"tokenized\"] = res\n",
    "\n",
    "all_words = []\n",
    "\n",
    "#NUMERO DOS: my method is to list every word in an array above, so we can count how many is similar from that giant array\n",
    "for index, row in df.iterrows():\n",
    "    for val in row[\"tokenized\"]:\n",
    "        x=''.join(c[0] for c in itertools.groupby(val))\n",
    "        all_words.append(x)\n",
    "\n",
    "#NUMERO TRES: we list the wordlists so we get LIT >:D\n",
    "wordlist = nltk.FreqDist(all_words)\n",
    "word_features = wordlist.keys()\n",
    "\n",
    "data = [] \n",
    "\n",
    "#NUMERO QUATRO: we count them words baby <3\n",
    "for word in word_features:\n",
    "    data.append([str(word), list(all_words).count(word)])\n",
    "\n",
    "countdf = pd.DataFrame(data, columns = ['Word', 'Count']) \n",
    "\n",
    "countdf.to_csv('WORD-COUNT-covid-sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# This is to simply sort the previous results :D\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "df = pd.read_csv('WORD-COUNT-covid-sentiment.csv')\n",
    "df.sort_values(by=\"Count\", ascending=False).to_csv('WORD-COUNT-SORTED-covid-sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>covid</td>\n",
       "      <td>41098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>perintah</td>\n",
       "      <td>40876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>33379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>indonesia</td>\n",
       "      <td>7473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>tangan</td>\n",
       "      <td>6872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32093</th>\n",
       "      <td>17076</td>\n",
       "      <td>17076</td>\n",
       "      <td>aisokong</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32094</th>\n",
       "      <td>17075</td>\n",
       "      <td>17075</td>\n",
       "      <td>bisar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32095</th>\n",
       "      <td>17074</td>\n",
       "      <td>17074</td>\n",
       "      <td>tuhdipakai</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32096</th>\n",
       "      <td>17073</td>\n",
       "      <td>17073</td>\n",
       "      <td>jewajiban</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32097</th>\n",
       "      <td>32097</td>\n",
       "      <td>32097</td>\n",
       "      <td>keluyuran</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32098 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Unnamed: 0.1        Word  Count\n",
       "0              11            11       covid  41098\n",
       "1               0             0    perintah  40876\n",
       "2              12            12          19  33379\n",
       "3              93            93   indonesia   7473\n",
       "4              27            27      tangan   6872\n",
       "...           ...           ...         ...    ...\n",
       "32093       17076         17076    aisokong      1\n",
       "32094       17075         17075       bisar      1\n",
       "32095       17074         17074  tuhdipakai      1\n",
       "32096       17073         17073   jewajiban      1\n",
       "32097       32097         32097   keluyuran      1\n",
       "\n",
       "[32098 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('WORD-COUNT-SORTED-covid-sentiment.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# DATAPREPROCESSING: The positive & negative vocabulary are actin' dirty on me yo. So we need to clean em up boi :O !\n",
    "\n",
    "posdf = pd.read_csv('positive.txt', names=[\"text\"])\n",
    "negdf = pd.read_csv('negative.txt', names=[\"text\"])\n",
    "\n",
    "posddf = dd.from_pandas(posdf, npartitions=30)\n",
    "negddf = dd.from_pandas(negdf, npartitions=30)\n",
    "\n",
    "def preProcess(row):\n",
    "    return stemmer.stem(row[\"text\"])\n",
    "\n",
    "posres = posddf.map_partitions(lambda posdf: posdf.apply(preProcess,axis=1)).compute(scheduler='processes')\n",
    "negres = negddf.map_partitions(lambda negdf: negdf.apply(preProcess,axis=1)).compute(scheduler='processes')\n",
    "\n",
    "posdf.drop([\"text\"], inplace=True, axis=1)\n",
    "posdf[\"cleaned\"] = posres\n",
    "posdf.drop_duplicates(subset =\"cleaned\",keep = False, inplace = True) \n",
    "\n",
    "negdf.drop([\"text\"], inplace=True, axis=1)\n",
    "negdf[\"cleaned\"] = negres\n",
    "negdf.drop_duplicates(subset =\"cleaned\",keep = False, inplace = True) \n",
    "\n",
    "posdf.to_csv('positive-cleaned.csv')\n",
    "negdf.to_csv('negative-cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postotal:  63260\n",
      "negtotal:  88512\n",
      "overall sentiment is negative by -39 %\n"
     ]
    }
   ],
   "source": [
    "# FINALE: we find the sentiment simply by comparing the total amount of bad words and good words\n",
    "\n",
    "posdf = pd.read_csv('positive-cleaned.csv')\n",
    "negdf = pd.read_csv('negative-cleaned.csv')\n",
    "df = pd.read_csv('WORD-COUNT-SORTED-covid-sentiment.csv')\n",
    "\n",
    "posarray = posdf[\"cleaned\"].to_numpy()\n",
    "poscondition = df[\"Word\"].isin(posarray)\n",
    "postotal = np.where(poscondition, df['Count'],0).sum()\n",
    "print(\"postotal: \", postotal)\n",
    "\n",
    "negarray = negdf[\"cleaned\"].to_numpy()\n",
    "negcondition = df[\"Word\"].isin(negarray)\n",
    "negtotal = np.where(negcondition, df['Count'],0).sum()\n",
    "print(\"negtotal: \", negtotal)\n",
    "\n",
    "if(negtotal>postotal):\n",
    "    print(\"overall sentiment is negative by\", int((postotal-negtotal)*100/min(postotal, negtotal)),\"%\" )\n",
    "else: \n",
    "    print(\"overall sentiment is positive by\", int((postotal-negtotal)*100/min(postotal, negtotal)),\"%\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
